# WER (%) Report:
# Total (2620 utterances): 2690/52576 = 5.12%, SUB/INS/DEL = 1985/330/375
# WER (%) Report:
# Total (2939 utterances): 6117/52343 = 11.69%, SUB/INS/DEL = 4697/706/714

nnet: xfmr

nnet_conf:
  input_size: 80
  enc_type: xfmr_rel
  enc_kwargs:
    att_dim: 512
    att_dropout: 0.2
    feedforward_dim: 2048
    proj_layer: conv2d
    radius: 256
    nhead: 8
    num_layers: 16
    pos_dropout: 0.2
    post_norm: false
    scale_embed: false
  dec_kwargs:
    att_dim: 512
    att_dropout: 0.2
    feedforward_dim: 2048
    nhead: 8
    num_layers: 8
    pos_dropout: 0.2
    scale_embed: false

asr_transform:
  aug_mask_zero: false
  aug_max_frame: 100
  aug_prob: 1
  feats: perturb-emph-fbank-log-aug
  frame_hop: 160
  frame_len: 400
  min_freq: 20
  num_aug_bands: 1
  num_aug_frame: 1
  num_mels: 80
  pre_emphasis: 0.97
  round_pow_of_two: true
  sr: 16000
  use_power: true
  window: hamm

task: ctc_xent
task_conf:
  ctc_weight: 0.2
  lsm_factor: 0.1

trainer_conf:
  clip_gradient: 5
  lr_scheduler: noam_lr
  lr_scheduler_kwargs:
    factor: 1
    transformer_dim: 512
    warmup: 10000
  lr_scheduler_period: step
  no_impr: 6
  no_impr_thres: 0.2
  optimizer: adam
  optimizer_kwargs:
    lr: 0.0
    weight_decay: 1.0e-05
  report_metrics: ["loss", "accu", "@ctc"]
  stop_criterion: accu

data_conf:
  fmt: am_raw
  loader:
    adapt_dur: 10
    adapt_token_num: 150
    max_dur: 30
    max_token_num: 400
    min_dur: 0.4
  train:
    text: data/librispeech/train/token
    utt2dur: data/librispeech/train/utt2dur
    wav_scp: data/librispeech/train/wav.scp
  valid:
    text: data/librispeech/dev/token
    utt2dur: data/librispeech/dev/utt2dur
    wav_scp: data/librispeech/dev/wav.scp
