# 4 GPU, DDP, 128 batch size
# WER (%) Report:
# Total (2620 utterances): 2810/52576 = 5.34%, SUB/INS/DEL = 2111/180/519
# WER (%) Report:
# Total (2939 utterances): 7003/52343 = 13.38%, SUB/INS/DEL = 5432/501/1070

nnet: "att"

nnet_conf:
  input_size: 80
  enc_type: "concat"
  enc_proj: 1024
  enc_kwargs:
    conv2d:
      out_features: -1
      channel: 48
      num_layers: 2
      stride: 2
      padding: 1
      kernel_size: 3
    variant_rnn:
      num_layers: 4
      hidden: 1024
      project: 512
      dropout: 0.3
      norm: "BN"
      bidirectional: true
      non_linear: "relu"
  dec_dim: 1024
  dec_kwargs:
    dec_rnn: "lstm"
    rnn_layers: 2
    rnn_hidden: 1024
    rnn_dropout: 0.2
    emb_dropout: 0.2
    dropout: 0.2
    input_feeding: true
    vocab_embeded: true
  att_type: "ctx"
  att_kwargs:
    att_dim: 1024

task: "ctc_xent"

task_conf:
  lsm_factor: 0.1
  ctc_weight: 0.2

asr_transform:
  feats: "emph-fbank-log-aug"
  frame_len: 400
  frame_hop: 160
  window: "hamm"
  round_pow_of_two: true
  use_power: true
  pre_emphasis: 0.97
  sr: 16000
  num_mels: 80
  min_freq: 20
  norm_mean: true
  norm_var: true
  aug_prob: 1
  aug_max_frame: 100
  aug_mask_zero: false
  num_aug_bands: 1
  num_aug_frame: 1

trainer_conf:
  optimizer: "adam"
  optimizer_kwargs:
    lr: 1.0e-3
    weight_decay: 1.0e-5
  lr_scheduler: "exp_lr"
  lr_scheduler_period: "step"
  lr_scheduler_kwargs:
    time_stamps: [500, 40000, 100000]
    peak_lr: 1.0e-3
    stop_lr: 1.0e-5
  no_impr: 6
  no_impr_thres: 0.1
  clip_gradient: 1
  report_metrics: ["loss", "accu", "@ctc"]
  stop_criterion: "accu"

data_conf:
  fmt: "am_raw"
  loader:
    max_token_num: 400
    adapt_token_num: 150
    max_dur: 30 # (s)
    min_dur: 0.4 # (s)
    adapt_dur: 10 # (s)
  train:
    text: data/librispeech/train/token
    utt2dur: data/librispeech/train/utt2dur
    wav_scp: data/librispeech/train/wav.scp
  valid:
    text: data/librispeech/dev/token
    utt2dur: data/librispeech/dev/utt2dur
    wav_scp: data/librispeech/dev/wav.scp
